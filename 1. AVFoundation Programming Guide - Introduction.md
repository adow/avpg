# 1. AVFoundation Programming Guide - Introduction

原文地址： [https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/00\_Introduction.html#//apple\_ref/doc/uid/TP40010188-CH1-SW3](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/00_Introduction.html#//apple_ref/doc/uid/TP40010188-CH1-SW3)

大纲

- 简介 Intruduction
	- 概览 At a Glance
	- 在 AVFoundation 中描述和使用媒体 (Representing and Using Media with AVFoundation)
	- 播放 Playback 
	- 读取，写入以及对 Assets 的重新编码 Reading, Writing, and Reencoding Assets 
	- 缩略图 Thumbnails 
	- 编辑 Editing
	- 拍照和视频采集 Still and Video Media Capture 
	- 使用 AVFoundation 进行并发编程 Concurrent Programming with AVFoundation
- 前提条件 Prerequisites
- 参考 See Also


## 简介 Intruduction

AVFoundation is one of several frameworks that you can use to play and create time-based audiovisual media. It provides an Objective-C interface you use to work on a detailed level with time-based audiovisual data. For example, you can use it to examine, create, edit, or reencode media files. You can also get input streams from devices and manipulate video during realtime capture and playback. Figure I-1 shows the architecture on iOS.

`AVFoundation` 是一个可以用来播放，创建基于时间的视听媒体的框架。他提供了 `Objective-C` 的接口来操作基于时间的视听数据底层。例如，你可以用来检测，创建，编辑，重新编码媒体文件。你可以从设备获取输入流，也可以在实时采集和播放视频的时候对他进行操作。

`AVFoundation` 在 iOS 上的架构

[img](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Art/frameworksBlockDiagram_2x.png)

在 OS X 上的架构

[img](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Art/frameworksBlockDiagramOSX_2x.png)

You should typically use the highest-level abstraction available that allows you to perform the tasks you want.

- If you simply want to play movies, use the AVKit framework.
- On iOS, to record video when you need only minimal control over format, use the UIKit framework (UIImagePickerController).

你应该优先使用最高层抽象的接口来实现需要的功能。

- 如果只是要简单的播放影片，使用 `AVKit` 框架;
- 在 `iOS` 上，如果在录像时只需要简单的控制格式，就使用 `UIKit` 框架中的 [UIImagePickerController](https://developer.apple.com/documentation/uikit/uiimagepickercontroller)。

Note, however, that some of the primitive data structures that you use in AV Foundation—including time-related data structures and opaque objects to carry and describe media data—are declared in the Core Media framework.

需要注意的是，在 `AVFoundation` 中使用的有些基础数据格式是在 `Core Media` 框架中被声明。 （包括一些时间相关的数据结构，以及一些用作传递和描述的不透明媒体数据格式）

### 概览 At a Glance

There are two facets to the AVFoundation framework—APIs related to video and APIs related just to audio. The older audio-related classes provide easy ways to deal with audio.

- To play sound files, you can use AVAudioPlayer.
- To record audio, you can use AVAudioRecorder.


`AVFoundation` 框架有两类：视频相关的 API 和音频相关的 API。有一类比较老的和音频相关的 API,他们提供了简单的音频操作方法。

- 如果要播放音频文件，可以直接使用 `AVAudioPlayer`;
- 如果要录音，直接使用 `AVAudioRecorder`;

You can also configure the audio behavior of your application using AVAudioSession; this is described in Audio Session Programming Guide.

你还可以通过 `AVAudioSession` 来在应用中配置音频相关的行为。他们在 [Audio Session Programming Guide](https://developer.apple.com/library/content/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40007875) 说明。

### 在 AVFoundation 中描述和使用媒体 (Representing and Using Media with AVFoundation)

The primary class that the AV Foundation framework uses to represent media is AVAsset. The design of the framework is largely guided by this representation. Understanding its structure will help you to understand how the framework works. An AVAsset instance is an aggregated representation of a collection of one or more pieces of media data (audio and video tracks). It provides information about the collection as a whole, such as its title, duration, natural presentation size, and so on. AVAsset is not tied to particular data format. AVAsset is the superclass of other classes used to create asset instances from media at a URL (see Using Assets) and to create new compositions (see Editing).

在 `AVFoundation` 框架中描述 (Representation) 媒体的最重要的类是 `AVAsseet`,他在设计这个框架的过程中起着非常重要的指导作用。理解 `AVAsset` 的结构将有助于你更好的了解这个框架是如何工作的。一个 `AVAsset` 实例是一个聚合的描述，他表示了一个或者多个媒体数据（音频和视频轨道）的集合。他提供的了一组整体的信息包括：标题，时长，原始显示尺寸等等。`AVAsset` 和具体的数据格式是无关的。通过继承 `AVAsset` 实现了一个通过 URL 来表示媒体的 `AVAsset`（ 参考 [Using Asset](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1))，同样也用这种方式来创建一个用来进行合成的 `AVAsset` (参考 [Editing](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/00_Introduction.html#//apple_ref/doc/uid/TP40010188-CH1-SW1))。

Each of the individual pieces of media data in the asset is of a uniform type and called a track. In a typical simple case, one track represents the audio component, and another represents the video component; in a complex composition, however, there may be multiple overlapping tracks of audio and video. Assets may also have metadata.

在 `AVAsset` 中的每个媒体信息片段都是一个统一的类型称为“轨道”。最常见的情况是，一个轨道表示音频，另一个轨道表示视频。但是在一些复合的媒体中，可能会存在多个重叠的音频和视频轨道。`AVAsset` 中还可能会包含元数据。

A vital concept in AV Foundation is that initializing an asset or a track does not necessarily mean that it is ready for use. It may require some time to calculate even the duration of an item (an MP3 file, for example, may not contain summary information). Rather than blocking the current thread while a value is being calculated, you ask for values and get an answer back asynchronously through a callback that you define using a block.

使用 `AVFoundation` 有一个非常的重要概念是，初始化的库 (`asset`) 或者轨道(`track`) 并不表示他们就已经是可用的了。有时他可能需要更多的时间来计算出时长（比如 MP3 文件就可能并没有包含摘要信息）。所以在需要计算的时候，要注意不要阻塞当前的线程，你可以通过 `block` 回调来异步查询需要的一个值。

相关章节: [Using Asset](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1), [Time and Media Representations](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/06_MediaRepresentations.html#//apple_ref/doc/uid/TP40010188-CH2-SW1)

#### 播放 Playback 

AVFoundation allows you to manage the playback of asset in sophisticated ways. To support this, it separates the presentation state of an asset from the asset itself. This allows you to, for example, play two different segments of the same asset at the same time rendered at different resolutions. The presentation state for an asset is managed by a player item object; the presentation state for each track within an asset is managed by a player item track object. Using the player item and player item tracks you can, for example, set the size at which the visual portion of the item is presented by the player, set the audio mix parameters and video composition settings to be applied during playback, or disable components of the asset during playback.

`AVFoundation` 可以让你以非常优雅的方式来管理播放。为了做到一点，他将`显示状态 (presentation state)` 从 `AVAsset` 中分离。因此，你可以在一个时间以两种不同的分辨率来播放同一个 `AVAsset` 的不同的片段。`asset` 中的`显示状态（presentation state)` 是被 `播放条目 (player item)` 管理的。在 `asset` 中的每个轨道的`显示状态（presentation state)` 是通过`轨道播放条目 (player item track)` 管理的。比如你可以通过 `播放条目 (player item)` 和 `轨道播放条目 (palyer item tracks)`来设置播放器中哪一部分内容可见，可以在播放的时候设置混音参数和进行视频合成，设置可以在播放的时候禁用某些 `asset` 中的组件。

You play player items using a player object, and direct the output of a player to the Core Animation layer. You can use a player queue to schedule playback of a collection of player items in sequence.

你可以用一个播放器来播放`播放条目 (player item)`,并把他直接输出到一个 `Core Animation` 的 图层 (`Layer`) 上面。你还可以使用一个 `播放队列 player queue` 协调一组`播放条目 (player item)`。

相关章节： [Playback](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/02_Playback.html#//apple_ref/doc/uid/TP40010188-CH3-SW1)

#### 读取，写入以及对 Assets 的重新编码 Reading, Writing, and Reencoding Assets 

AVFoundation allows you to create new representations of an asset in several ways. You can simply reencode an existing asset, or—in iOS 4.1 and later—you can perform operations on the contents of an asset and save the result as a new asset.

`AVFoundation` 让你可以用多种方法来创建一个新的 `Asset` 描述 (`Representation)`。你可以对对一个已有的 `Asset` 进行重新编码。在 `iOS 4.1` 之后，你可以对一个 `Asset` 中的内容进行一些操作，然后把他保存为一个新的 `Asset`.

You use an export session to reencode an existing asset into a format defined by one of a small number of commonly-used presets. If you need more control over the transformation, in iOS 4.1 and later you can use an asset reader and asset writer object in tandem to convert an asset from one representation to another. Using these objects you can, for example, choose which of the tracks you want to be represented in the output file, specify your own output format, or modify the asset during the conversion process.

通过一些通用的预设好的参数，你可以使用 `export session` 来对一个已有的 `Asset` 重新编码.在 `iOS 4.1` 之后，你可以控制这个转换过程，在将一个 `asset` 描述 (`representation`) 转换到另一个的过程中，通过串联 (`in tendem`)使用 `asset reader` 和 `asset write` 对象来实现。通过他们，你可以在转换的过程中选择有那些轨道可以被输出到目标文件中，可以指定自己的输出格式，甚至可以修改 `asset`。

To produce a visual representation of the waveform, you use an asset reader to read the audio track of an asset.

你可以用 `asset reader` 来读取一个 `asset` 中的音频音轨来显示波形图。

相关章节：[Using Assets](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1)

#### 缩略图 Thumbnails 

To create thumbnail images of video presentations, you initialize an instance of `AVAssetImageGenerator` using the asset from which you want to generate thumbnails. AVAssetImageGenerator uses the default enabled video tracks to generate images.

要创建 视频的缩略图的话，可以用 `AVAsset` 来创建一个 [AVAssetImageGenerator](https://developer.apple.com/documentation/avfoundation/avassetimagegenerator) 实例，他从默认的视频轨道来创建缩略图。

相关章节: [Using Assets](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW1)

####  编辑 Editing 

AVFoundation uses compositions to create new assets from existing pieces of media (typically, one or more video and audio tracks). You use a mutable composition to add and remove tracks, and adjust their temporal orderings. You can also set the relative volumes and ramping of audio tracks; and set the opacity, and opacity ramps, of video tracks. A composition is an assemblage of pieces of media held in memory. When you export a composition using an export session, it’s collapsed to a file.

`AVFoundation` 通过合成器 （`compositions`） 来把一些媒体片段来组合成新的 `asset`（一般会包含一个或者多个视频，音频轨道）。你可以通过一个可变的合成器来移除或者添加轨道，或者调整他们的时序。你还可以设置音轨的音量和调速，或者修改视频轨道的透明度和 `opacity ramps`。合成器 `compositions` 在内存中组合了多个每条片段，然后通过 `export session` 输出到一个外部文件 (`collapsed to a file`)。

You can also create an asset from media such as sample buffers or still images using an asset writer.

你还可以通过使用 `Asset Writer` 来将一些媒体信息（比如 `sample buffers` 和静态文件）来创建一个 `Asset`.

相关章节 [Editing](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/03_Editing.html#//apple_ref/doc/uid/TP40010188-CH8-SW1)

#### 拍照和视频采集 Still and Video Media Capture 

Recording input from cameras and microphones is managed by a capture session. A capture session coordinates the flow of data from input devices to outputs such as a movie file. You can configure multiple inputs and outputs for a single session, even when the session is running. You send messages to the session to start and stop data flow.

通过 `Capture Session` 用来管理摄像头和麦克风的采集过程。`Capture Session` 可用来协调输入设备和输出文件（比如电影文件）之间的数据流。在一个 `Capture Session` 中，你可以配置一个或者多个输入输出，即使这是正在运行的过程中。通过发送 `start` 和 `stop` 消息给 `Capture Session` 来开始和结束数据流。

In addition, you can use an instance of a `preview layer` to show the user what a camera is recording.

此外，你可以创建一个预览图层 (`preview layer`) 把摄像头采集的内容播放给用户。

相关章节 [Still and Video Media Capture](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/04_MediaCapture.html#//apple_ref/doc/uid/TP40010188-CH5-SW2)

#### 使用 AVFoundation 进行并发编程 Concurrent Programming with AVFoundation

Callbacks from AVFoundation—invocations of blocks, key-value observers, and notification handlers—are not guaranteed to be made on any particular thread or queue. Instead, AVFoundation invokes these handlers on threads or queues on which it performs its internal tasks.

`AVFoundation` 的回调（`block` 调用，`key-value observers`, 以及 `notification` 处理函数）并没有保证在某个特定的线程或者队列上执行。AVFoundation 在这些线程和队列上执行他的具体内部内部任务。

There are two general guidelines as far as notifications and threading:

- UI related notifications occur on the main thread.
- Classes or methods that require you create and/or specify a queue will return notifications on that queue.


对于使用 `notification` 和 `threading` 有两个指南：

- UI 相关的通知都是在主线程发生的；
- 你创建或者指定哪个队列就会从那个队列返回通知（似乎不对）。

Beyond those two guidelines (and there are exceptions, which are noted in the reference documentation) you should not assume that a notification will be returned on any specific thread.

除了以上两个原则（还有一些例外，他们会被列出引用），你不应该假定通知会在任何指定的线程上面返回。

If you’re writing a multithreaded application, you can use the NSThread method `isMainThread` or `[[NSThread currentThread] isEqual:<#A stored thread reference#>]` to test whether the invocation thread is a thread you expect to perform your work on. You can redirect messages to appropriate threads using methods such as `performSelectorOnMainThread:withObject:waitUntilDone: andperformSelect` or `:onThread:withObject:waitUntilDone:modes:`. You could also use `dispatch_async` to “bounce” to your blocks on an appropriate queue, either the main queue for UI tasks or a queue you have up for concurrent operations. For more about concurrent operations, see Concurrency Programming Guide; for more about blocks, see Blocks Programming Topics. The AVCam-iOS: Using AVFoundation to Capture Images and Movies sample code is considered the primary example for all AVFoundation functionality and can be consulted for examples of thread and queue usage with AVFoundation.

如果你在写一个多线程的应用，你可以是用 `NSThread` 的 `isMainThread` 或者 `[[NSThread currentThread] isEqual:<#A stored thread reference#>]` 来检查是否在你需要的线程上调用。你可以通过 `performSelectorOnMainThread:withObject:waitUntilDone: andperformSelect` 或者 `:onThread:withObject:waitUntilDone:modes:` 来像其他线程转发消息。你也可以使用 `dispath_async` 将代码转移 `bounce` 到合适的队列上执行，要么在主队列执行的 UI 任务，要么在其他队列上执行的并发操作。关于更多并发操作，参考 [Concurrency Programming Guide](https://developer.apple.com/library/content/documentation/General/Conceptual/ConcurrencyProgrammingGuide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40008091)。关于 blocks，参考 [ Blocks Programming Topics](https://developer.apple.com/library/content/documentation/Cocoa/Conceptual/Blocks/Articles/00_Introduction.html#//apple_ref/doc/uid/TP40007502).在示例代码 [AVCam-iOS: Using AVFoundation to Capture Images and Movies](https://developer.apple.com/library/content/samplecode/AVCam/Introduction/Intro.html#//apple_ref/doc/uid/DTS40010112) 中包含了如何使用线程和队列的例子。

### 前提条件 Prerequisites

AVFoundation is an advanced Cocoa framework. To use it effectively, you must have:

- A solid understanding of fundamental Cocoa development tools and techniques
- A basic grasp of blocks
- A basic understanding of key-value coding and key-value observing
- For playback, a basic understanding of Core Animation (see Core Animation Programming Guide or, for basic playback, the AVKit Framework Reference.

`AVFoundation` 是一个高阶的 Cocoa 框架，要有效的使用他，需要具备以下能力：

- 对 `Cocoa` 开发工具和技术有坚实的理解；
- 理解 `blocks`；
- 对 `key-value` 编程 和 `key-value observing` 有基础理解；
- 对于播放，需要简单的了解一下 Core Animation (参考: [Core Animation Programming Guide](https://developer.apple.com/library/content/documentation/Cocoa/Conceptual/CoreAnimation_guide/Introduction/Introduction.html#//apple_ref/doc/uid/TP40004514))，也可以使用 `AVKit` 实现简单的播放功能 (参考 [AVKit Framework Reference](https://developer.apple.com/documentation/avkit).

### 参考 See Also

There are several AVFoundation examples including two that are key to understanding and implementation Camera capture functionality:

- AVCam-iOS: Using AVFoundation to Capture Images and Movies is the canonical sample code for implementing any program that uses the camera functionality. It is a complete sample, well documented, and covers the majority of the functionality showing the best practices.
- AVCamManual: Extending AVCam to Use Manual Capture API is the companion application to AVCam. It implements Camera functionality using the manual camera controls. It is also a complete example, well documented, and should be considered the canonical example for creating camera applications that take advantage of manual controls.
- RosyWriter is an example that demonstrates real time frame processing and in particular how to apply filters to video content. This is a very common developer requirement and this example covers that functionality.
- AVLocationPlayer: Using AVFoundation Metadata Reading APIs demonstrates using the metadata APIs.

`AVFoundation` 示例代码对于理解和实现摄像头捕捉功能很有帮助。

- [AVCam-iOS](https://developer.apple.com/library/content/samplecode/AVCam/Introduction/Intro.html#//apple_ref/doc/uid/DTS40010112)： 对于使用 AVFoundation 拍摄照片和视频是很权威的示例代码。他包含了完整的示例，文档，覆盖了主要使用功能，并提供了最好的实践。
- [AVCamManual: 扩展了 AVCam](https://developer.apple.com/library/content/samplecode/AVCamManual/Introduction/Intro.html#//apple_ref/doc/uid/TP40014578)，使用了视频捕捉 API 的手动功能。他也提供了完整的示例，文档，对于实现相机的手动功能是一个很好的示范。
- [RosyWriter](https://developer.apple.com/library/content/samplecode/RosyWriter/Introduction/Intro.html#//apple_ref/doc/uid/DTS40011110) 是一个用来展示如何实时进行帧处理（特别是视频滤镜）的示例。对于这里应用有很多需求。
- [AVLocationPlayer](https://developer.apple.com/library/content/samplecode/AVLocationPlayer/Introduction/Intro.html#//apple_ref/doc/uid/TP40014495): 示例如何使用 AVFoundation Metadata Reading APIs 来获取元数据。