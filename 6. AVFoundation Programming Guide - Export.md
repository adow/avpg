# 6. AVFoundation Programming Guide - Export

原文地址：[https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/05\_Export.html#//apple\_ref/doc/uid/TP40010188-CH9-SW2](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/05_Export.html#//apple_ref/doc/uid/TP40010188-CH9-SW2)

大纲：

- 读取 Asset Reading an Asset
	- 创建 asset 读取对象 Creating the Asset Reader
	- 建立一个输出来获取内容 Setting Up the Asset Reader Outputs
	- 从 Asset 中读取媒体数据 Reading the Asset’s Media Data
- 写入 Asset Writing an Asset
	- 创建一个写入器 Creating the Asset Writer
	- 创建写入器的输入 Setting Up the Asset Writer Inputs
	- 写入媒体数据 Writing Media Data
- 重新编码 Reencoding Assets
- 整合在一起：使用读取器和写入器协同工作来对库进行重新编码 Putting It All Together: Using an Asset Reader and Writer in Tandem to Reencode an Asset
	- 初始化的处理 Handling the Initial Setup
	- 初始化读取器和写入器 Initializing the Asset Reader and Writer
	- 转码 Reencoding the Asset
	- 结束时的处理 Handling Completion
	- 取消时的处理 Handling Cancellation
	- 库的输出设置助手 Asset Output Settings Assistant

## 导出 Export

To read and write audiovisual assets, you must use the export APIs provided by the AVFoundation framework. The [AVAssetExportSession](https://developer.apple.com/documentation/avfoundation/avassetexportsession) class provides an interface for simple exporting needs, such as modifying the file format or trimming the length of an asset (see [Trimming and Transcoding a Movie](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW8)). For more in-depth exporting needs, use the [AVAssetReader](https://developer.apple.com/documentation/avfoundation/avassetreader) and [AVAssetWriter](https://developer.apple.com/documentation/avfoundation/avassetwriter) classes.

要想读取和写入音视频 asset, 你必须要使用 `AVFoundation` 框架提供的导出 api。`AVAssetExportSession` 类提供了导出时一些简单操作，类似修改文件格式，剪裁内容等（参考 [Trimming and Transcoding a Movie](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/01_UsingAssets.html#//apple_ref/doc/uid/TP40010188-CH7-SW8))。如果需要更深入的导出功能 ，那就要用到 [AVAssetReader](https://developer.apple.com/documentation/avfoundation/avassetreader) 和[AVAssetWriter](https://developer.apple.com/documentation/avfoundation/avassetwriter)。

Use an `AVAssetReader` when you want to perform an operation on the contents of an asset. For example, you might read the audio track of an asset to produce a visual representation of the waveform. To produce an asset from media such as sample buffers or still images, use an AVAssetWriter object.

通过使用 `AVAssetReader` 可以对 `asset` 的内容做一些操作。比如可以从 `asset` 中读取音轨并显示为波形。你也可以使用 `AVAssetWriter` 将 `sampe buffers` 或者静止的图像 （`still images`） 输出到一个 asset 中。

Note: The asset reader and writer classes are not intended to be used for real-time processing. In fact, an asset reader cannot even be used for reading from a real-time source like an HTTP live stream. However, if you are using an asset writer with a real-time data source, such as an [AVCaptureOutput](https://developer.apple.com/documentation/avfoundation/avcaptureoutput) object, set the [expectsMediaDataInRealTime](https://developer.apple.com/documentation/avfoundation/avassetwriterinput/1387827-expectsmediadatainrealtime) property of your asset writer’s inputs to YES. Setting this property to YES for a non-real-time data source will result in your files not being interleaved properly.

提示：关于 `asset` 的这两个读取写入对象都不是被用来进行实时处理的。事实上，`asset reader` 是无法从类似 HTTP 直播流上面读取内容的。但是，如果你在类似 [AVCaptureOutput](https://developer.apple.com/documentation/avfoundation/avcaptureoutput) 对象中将 `asset writer`  [expectsMediaDataInRealTime](https://developer.apple.com/documentation/avfoundation/avassetwriterinput/1387827-expectsmediadatainrealtime) 属性设置为 YES，那还是可以用他来写入实时数据的。在非实时的数据源中开启这个设置反而会导致文件中的内容没有被正确的写入。

### 读取 Asset Reading an Asset

Each `AVAssetReader` object can be associated only with a single asset at a time, but this asset may contain multiple tracks. For this reason, you must assign concrete subclasses of the [AVAssetReaderOutput](https://developer.apple.com/documentation/avfoundation/avassetreaderoutput) class to your asset reader before you begin reading in order to configure how the media data is read. There are three concrete subclasses of the `AVAssetReaderOutput` base class that you can use for your asset reading needs: [AVAssetReaderTrackOutput](https://developer.apple.com/documentation/avfoundation/avassetreadertrackoutput), [AVAssetReaderAudioMixOutput](https://developer.apple.com/documentation/avfoundation/avassetreaderaudiomixoutput), and [AVAssetReaderVideoCompositionOutput](https://developer.apple.com/documentation/avfoundation/avassetreadervideocompositionoutput).

每个 `AVAssetReader` 对象一次只能操作一个 `asset`，但是他可以包含多个轨道。所以当你开始读取前，你需要使用一个具体的 [AVAssetReaderOutput](https://developer.apple.com/documentation/avfoundation/avassetreaderoutput) 子类来配置如何具体的读取媒体内容。他有三个具体的子类： [AVAssetReaderTrackOutput](https://developer.apple.com/documentation/avfoundation/avassetreadertrackoutput), [AVAssetReaderAudioMixOutput](https://developer.apple.com/documentation/avfoundation/avassetreaderaudiomixoutput) 和  [AVAssetReaderVideoCompositionOutput](https://developer.apple.com/documentation/avfoundation/avassetreadervideocompositionoutput)。

#### 创建 asset 读取对象 Creating the Asset Reader

All you need to initialize an `AVAssetReader` object is the asset that you want to read.

你只需要初始化一个 `AVAssetReader` 对象

	NSError *outError;
	AVAsset *someAsset = <#AVAsset that you want to read#>;
	AVAssetReader *assetReader = [AVAssetReader assetReaderWithAsset:someAsset error:&outError];
	BOOL success = (assetReader != nil);

Note: Always check that the asset reader returned to you is non-nil to ensure that the asset reader was initialized successfully. Otherwise, the error parameter (`outError` in the previous example) will contain the relevant error information.

注意：你应该每次都检查这个对象是否被成功的创建了(是否是 `nil`)。失败的时候会在 `outError` 中包含相关的出错信息。

#### 建立一个输出来获取内容 Setting Up the Asset Reader Outputs

After you have created your asset reader, set up at least one output to receive the media data being read. When setting up your outputs, be sure to set the [alwaysCopiesSampleData](https://developer.apple.com/documentation/avfoundation/avassetreaderoutput/1389189-alwayscopiessampledata) property to NO. In this way, you reap the benefits of performance improvements. In all of the examples within this chapter, this property could and should be set to NO.

在你创建好读取 (`asset reader`) 之后，你至少要建立一个输出 (`output`) 来接收读取到的媒体数据。在建立输出的时候，你要将  [alwaysCopiesSampleData](https://developer.apple.com/documentation/avfoundation/avassetreaderoutput/1389189-alwayscopiessampledata) 设置为 NO，以此来获得更好的性能提升。这里所有的示例代码中的这个输出都被设置为 NO。

If you want only to read media data from one or more tracks and potentially convert that data to a different format, use the `AVAssetReaderTrackOutput` class, using a single track output object for each [AVAssetTrack](https://developer.apple.com/documentation/avfoundation/avassettrack) object that you want to read from your asset. To decompress an audio track to Linear PCM with an asset reader, you set up your track output as follows:

如果你要将一个或多个轨道的数据转换格式，你只要为每个轨道 (`AVAssetTrack`) 对象建立一个 `AVAssetReaderTrackOutput` 输出来读取数据。如果要将读取后的内容进行音频解码到线性 PCM (`Linear PCM`)，那你要用下面的方式：

	AVAsset *localAsset = assetReader.asset;
	// Get the audio track to read.
	AVAssetTrack *audioTrack = [[localAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0];
	// Decompression settings for Linear PCM
	NSDictionary *decompressionAudioSettings = @{ AVFormatIDKey : [NSNumber numberWithUnsignedInt:kAudioFormatLinearPCM] };
	// Create the output with the audio track and decompression settings.
	AVAssetReaderOutput *trackOutput = [AVAssetReaderTrackOutput assetReaderTrackOutputWithTrack:audioTrack outputSettings:decompressionAudioSettings];
	// Add the output to the reader if possible.
	if ([assetReader canAddOutput:trackOutput])
	    [assetReader addOutput:trackOutput];

Note: To read the media data from a specific asset track in the format in which it was stored, pass `nil` to the `outputSettings` parameter.

提示：要从指定的轨道上读取他现有的格式媒体的话，`outputSettings` 要设置为 `nil`。

You use the `AVAssetReaderAudioMixOutput` and `AVAssetReaderVideoCompositionOutput` classes to read media data that has been mixed or composited together using an [AVAudioMix](https://developer.apple.com/documentation/avfoundation/avaudiomix) object or [AVVideoComposition](https://developer.apple.com/documentation/avfoundation/avvideocomposition) object, respectively. Typically, these outputs are used when your asset reader is reading from an [AVComposition](https://developer.apple.com/documentation/avfoundation/avcomposition) object.

经过[AVAudioMix](https://developer.apple.com/documentation/avfoundation/avaudiomix) 合成器对象合成的媒体内容要用 `AVAssetReaderAudioMixOutput` 读取， 用 [AVVideoComposition](https://developer.apple.com/documentation/avfoundation/avvideocomposition) 视频合成器合成的媒体内容要用 `AVAssetReaderVideoCompositionOutput` 进行读取。一般这些输出对象都用于来自 [AVComposition](https://developer.apple.com/documentation/avfoundation/avcomposition) 合成的 `asset` 的读取。

With a single audio mix output, you can read multiple audio tracks from your asset that have been mixed together using an `AVAudioMix` object. To specify how the audio tracks are mixed, assign the mix to the `AVAssetReaderAudioMixOutput` object after initialization. The following code displays how to create an audio mix output with all of the audio tracks from your asset, decompress the audio tracks to Linear PCM, and assign an audio mix object to the output. For details on how to configure an audio mix, see [Editing](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/03_Editing.html#//apple_ref/doc/uid/TP40010188-CH8-SW1).

通过一个音频 mix 输出 (`audio mix output`)，你就可以将 asset 中使用 `AVAudioMix` 对象合成的多个音频轨道读取出来。你要在 `AVAssetReaderAudioMixOutput` 对象初始化之后设置 `mix` 属性来告诉他音频是如何合成的。下面的代码示例了如何为 `asset` 中所有的音频轨道 (`audio tracks`) 创建一个音频合成输出 (`audio mix output`)，将音频解码为 `Linear PCM`，然后将 `audio mix` 对象设置为输出。 要知道关于配置音频合成的更多细节，请参考 [Editing](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/03_Editing.html#//apple_ref/doc/uid/TP40010188-CH8-SW1).

	AVAudioMix *audioMix = <#An AVAudioMix that specifies how the audio tracks from the AVAsset are mixed#>;
	// Assumes that assetReader was initialized with an AVComposition object.
	AVComposition *composition = (AVComposition *)assetReader.asset;
	// Get the audio tracks to read.
	NSArray *audioTracks = [composition tracksWithMediaType:AVMediaTypeAudio];
	// Get the decompression settings for Linear PCM.
	NSDictionary *decompressionAudioSettings = @{ AVFormatIDKey : [NSNumber numberWithUnsignedInt:kAudioFormatLinearPCM] };
	// Create the audio mix output with the audio tracks and decompression setttings.
	AVAssetReaderOutput *audioMixOutput = [AVAssetReaderAudioMixOutput assetReaderAudioMixOutputWithAudioTracks:audioTracks audioSettings:decompressionAudioSettings];
	// Associate the audio mix used to mix the audio tracks being read with the output.
	audioMixOutput.audioMix = audioMix;
	// Add the output to the reader if possible.
	if ([assetReader canAddOutput:audioMixOutput])
	    [assetReader addOutput:audioMixOutput];
	

Note: Passing `nil` for the `audioSettings` parameter tells the asset reader to return samples in a convenient uncompressed format. The same is true for the `AVAssetReaderVideoCompositionOutput` class.

提示：将 `audioSettings` 参数设置为 `nil`以此来告诉 `asset reader` 返回未解码的采样信息。对于 `AVAssetReaderVideoCompositionOutput` 也一样。

The video composition output behaves in much the same way: You can read multiple video tracks from your asset that have been composited together using an `AVVideoComposition` object. To read the media data from multiple composited video tracks and decompress it to ARGB, set up your output as follows:

视频合成器 (`video composition`) 的输出也是类似的方法：你可以用 `AVVideoComposition` 对象将已经合成的多个视频轨道读取出来。下面的代码示例了如何读取已经合成的多个视频轨道并解码到 `ARGB` 格式：

	AVVideoComposition *videoComposition = <#An AVVideoComposition that specifies how the video tracks from the AVAsset are composited#>;
	// Assumes assetReader was initialized with an AVComposition.
	AVComposition *composition = (AVComposition *)assetReader.asset;
	// Get the video tracks to read.
	NSArray *videoTracks = [composition tracksWithMediaType:AVMediaTypeVideo];
	// Decompression settings for ARGB.
	NSDictionary *decompressionVideoSettings = @{ (id)kCVPixelBufferPixelFormatTypeKey : [NSNumber numberWithUnsignedInt:kCVPixelFormatType_32ARGB], (id)kCVPixelBufferIOSurfacePropertiesKey : [NSDictionary dictionary] };
	// Create the video composition output with the video tracks and decompression setttings.
	AVAssetReaderOutput *videoCompositionOutput = [AVAssetReaderVideoCompositionOutput assetReaderVideoCompositionOutputWithVideoTracks:videoTracks videoSettings:decompressionVideoSettings];
	// Associate the video composition used to composite the video tracks being read with the output.
	videoCompositionOutput.videoComposition = videoComposition;
	// Add the output to the reader if possible.
	if ([assetReader canAddOutput:videoCompositionOutput])
	    [assetReader addOutput:videoCompositionOutput];
	

#### 从 Asset 中读取媒体数据 Reading the Asset’s Media Data

To start reading after setting up all of the outputs you need, call the [startReading](https://developer.apple.com/documentation/avfoundation/avassetreader/1390286-startreading) method on your asset reader. Next, retrieve the media data individually from each output using the [copyNextSampleBuffer](https://developer.apple.com/documentation/avfoundation/avassetreaderoutput/1385732-copynextsamplebuffer) method. To start up an asset reader with a single output and read all of its media samples, do the following:

通过调用 `asset reader` 的  `startReading` 让所有的输出开始读取。然后为每个输出调用 [copyNextSampleBuffer](https://developer.apple.com/documentation/avfoundation/avassetreaderoutput/1385732-copynextsamplebuffer) 来获取媒体数据 (`media data`)。下面的代码用一个输出从 `asset reader` 中读取所有的媒体数据 (`media data`)。

	// Start the asset reader up.
	[self.assetReader startReading];
	BOOL done = NO;
	while (!done)
	{
	  // Copy the next sample buffer from the reader output.
	  CMSampleBufferRef sampleBuffer = [self.assetReaderOutput copyNextSampleBuffer];
	  if (sampleBuffer)
	  {
	    // Do something with sampleBuffer here.
	    CFRelease(sampleBuffer);
	    sampleBuffer = NULL;
	  }
	  else
	  {
	    // Find out why the asset reader output couldn't copy another sample buffer.
	    if (self.assetReader.status == AVAssetReaderStatusFailed)
	    {
	      NSError *failureError = self.assetReader.error;
	      // Handle the error here.
	    }
	    else
	    {
	      // The asset reader output has read all of its samples.
	      done = YES;
	    }
	  }
	}
	

### 写入 Asset Writing an Asset

The [AVAssetWriter](https://developer.apple.com/documentation/avfoundation/avassetwriter) class to write media data from multiple sources to a single file of a specified file format. You don’t need to associate your asset writer object with a specific asset, but you must use a separate asset writer for each output file that you want to create. Because an asset writer can write media data from multiple sources, you must create an [AVAssetWriterInput](https://developer.apple.com/documentation/avfoundation/avassetwriterinput) object for each individual track that you want to write to the output file. Each `AVAssetWriterInput` object expects to receive data in the form of [CMSampleBufferRef](https://developer.apple.com/documentation/coremedia/cmsamplebufferref) objects, but if you want to append [CVPixelBufferRef](https://developer.apple.com/documentation/corevideo/cvpixelbufferref) objects to your asset writer input, use the [AVAssetWriterInputPixelBufferAdaptor](https://developer.apple.com/documentation/avfoundation/avassetwriterinputpixelbufferadaptor) class.

[AVAssetWriter](https://developer.apple.com/documentation/avfoundation/avassetwriter) 类用于将多个数据源的媒体数据 (`media data`) 写入到指定格式的文件中。你不必为每个 `asset` 都指定一个写入器对象 (`asset write`),但是你必须为每个输出文件都创建一个写入器。因为一个写入器可以从多个数据源写入媒体数据，你要为每个轨道来创建一个 [AVAssetWriterInput](https://developer.apple.com/documentation/avfoundation/avassetwriterinput) 对象来将他们写入到输出文件中。每个 [AVAssetWriterInput](https://developer.apple.com/documentation/avfoundation/avassetwriterinput) 对象一般获取到的是 [CMSampleBufferRef](https://developer.apple.com/documentation/coremedia/cmsamplebufferref) 对象的格式的数据，如果你要追加 [CVPixelBufferRef](https://developer.apple.com/documentation/corevideo/cvpixelbufferref) 对象到 [AVAssetWriterInput](https://developer.apple.com/documentation/avfoundation/avassetwriterinput) 中，你需要使用 [AVAssetWriterInputPixelBufferAdaptor](https://developer.apple.com/documentation/avfoundation/avassetwriterinputpixelbufferadaptor)。

#### 创建一个写入器 Creating the Asset Writer

To create an asset writer, specify the URL for the output file and the desired file type. The following code displays how to initialize an asset writer to create a QuickTime movie:

你需要得到输出文件的地址 URL 和指定的文件类型来创建一个写入器 (`asset writer`)。下面的代码示例了如何来创建一个用来输出 `QuickTime` 影片的写入器:

	NSError *outError;
	NSURL *outputURL = <#NSURL object representing the URL where you want to save the video#>;
	AVAssetWriter *assetWriter = [AVAssetWriter assetWriterWithURL:outputURL
	                                                      fileType:AVFileTypeQuickTimeMovie
	                                                         error:&outError];
	BOOL success = (assetWriter != nil);
	

#### 创建写入器的输入 Setting Up the Asset Writer Inputs

For your asset writer to be able to write media data, you must set up at least one asset writer input. For example, if your source of media data is already vending media samples as `CMSampleBufferRef` objects, just use the `AVAssetWriterInput` class. To set up an asset writer input that compresses audio media data to 128 kbps AAC and connect it to your asset writer, do the following:

在用写入器 `asset writer` 写入媒体数据前，你还需要为他至少建立一个`写入器输入` (`asset writer input`)。如果你已经是 `CMSapleBufferRef` 对象的采样数据，那只要使用 `AVAssetWriterInput`。下面的代码创建的写入器输入 （`asset writer input`） 将音频数据编码为 `128 kbps AAC`，然后和写入器 （`asset writer`） 连接 （`connect`） 起来：

	// Configure the channel layout as stereo.
	AudioChannelLayout stereoChannelLayout = {
	    .mChannelLayoutTag = kAudioChannelLayoutTag_Stereo,
	    .mChannelBitmap = 0,
	    .mNumberChannelDescriptions = 0
	};
	 
	// Convert the channel layout object to an NSData object.
	NSData *channelLayoutAsData = [NSData dataWithBytes:&stereoChannelLayout length:offsetof(AudioChannelLayout, mChannelDescriptions)];
	 
	// Get the compression settings for 128 kbps AAC.
	NSDictionary *compressionAudioSettings = @{
	    AVFormatIDKey         : [NSNumber numberWithUnsignedInt:kAudioFormatMPEG4AAC],
	    AVEncoderBitRateKey   : [NSNumber numberWithInteger:128000],
	    AVSampleRateKey       : [NSNumber numberWithInteger:44100],
	    AVChannelLayoutKey    : channelLayoutAsData,
	    AVNumberOfChannelsKey : [NSNumber numberWithUnsignedInteger:2]
	};
	 
	// Create the asset writer input with the compression settings and specify the media type as audio.
	AVAssetWriterInput *assetWriterInput = [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeAudio outputSettings:compressionAudioSettings];
	// Add the input to the writer if possible.
	if ([assetWriter canAddInput:assetWriterInput])
	    [assetWriter addInput:assetWriterInput];

Note: If you want the media data to be written in the format in which it was stored, pass `nil` in the `outputSettings` parameter. Pass `nil `only if the asset writer was initialized with a `fileType` of `AVFileTypeQuickTimeMovie`.

提示：如果你要他原来格式存储的媒体数据，将 `outputSettings` 参数设置为 `nil`。只有用 `AVFileTypeQuickTimeMove` 格式来创建写入器 `asset writer` 的时候才传递 `nil`。

Your asset writer input can optionally include some metadata or specify a different transform for a particular track using the [metadata](https://developer.apple.com/documentation/avfoundation/avassetwriterinput/1386328-metadata) and [transform](https://developer.apple.com/documentation/avfoundation/avassetwriterinput/1390183-transform) properties respectively. For an asset writer input whose data source is a video track, you can maintain the video’s original transform in the output file by doing the following:

你的写入器输入 （`asset writer input`） 可以有选择的对特定的轨道 （`track`） 包含一些元数据（使用 [metadata](https://developer.apple.com/documentation/avfoundation/avassetwriterinput/1386328-metadata) 属性），也可以进行一些不同的变形 (使用 [transform](https://developer.apple.com/documentation/avfoundation/avassetwriterinput/1390183-transform) 属性)。对于视频轨道 （`video track`） 的写入器输入 （`asset writer input`），你可以通过下面的代码在输出文件中保留原来的变形设置：

	AVAsset *videoAsset = <#AVAsset with at least one video track#>;
	AVAssetTrack *videoAssetTrack = [[videoAsset tracksWithMediaType:AVMediaTypeVideo] objectAtIndex:0];
	assetWriterInput.transform = videoAssetTrack.preferredTransform;

Note: Set the `metadata` and `transform` properties before you begin writing with your asset writer for them to take effect.

提示：要在写入器（`asset writer`） 写入数据前设置 `metadata` 和 `transform` 属性才会起作用。

When writing media data to the output file, sometimes you may want to allocate pixel buffers. To do so, use the `AVAssetWriterInputPixelBufferAdaptor` class. For greatest efficiency, instead of adding pixel buffers that were allocated using a separate pool, use the pixel buffer pool provided by the pixel buffer adaptor. The following code creates a pixel buffer object working in the RGB domain that will use [CGImage](https://developer.apple.com/documentation/uikit/uiimage/1624159-cgimage) objects to create its pixel buffers.

在你将媒体数据 （`media data`） 输出到文件的时候，有时你会需要分配一些 `pixel buffers`。这时你可以使用 `AVAssetWriterInputPixelBufferAdaptor`。为了最好的效率，你应该通过 `pixel buffer adaptor` 来使用 `pixel buffer pool`，而不是每次单独创建一个 `pixel buffer pool`。下面的代码向你演示了一个在 `RGB` 域下面工作的 `pixel buffer` 对象，他使用 [CGImage](https://developer.apple.com/documentation/uikit/uiimage/1624159-cgimage) 对象来创建他的 `pixel buffers`。

	NSDictionary *pixelBufferAttributes = @{
	     kCVPixelBufferCGImageCompatibilityKey : [NSNumber numberWithBool:YES],
	     kCVPixelBufferCGBitmapContextCompatibilityKey : [NSNumber numberWithBool:YES],
	     kCVPixelBufferPixelFormatTypeKey : [NSNumber numberWithInt:kCVPixelFormatType_32ARGB]
	};
	AVAssetWriterInputPixelBufferAdaptor *inputPixelBufferAdaptor = [AVAssetWriterInputPixelBufferAdaptor assetWriterInputPixelBufferAdaptorWithAssetWriterInput:self.assetWriterInput sourcePixelBufferAttributes:pixelBufferAttributes];

Note: All `AVAssetWriterInputPixelBufferAdaptor` objects must be connected to a single asset writer input. That asset writer input must accept media data of type [AVMediaTypeVideo](https://developer.apple.com/documentation/avfoundation/avmediatypevideo).

提示：所有的 `AVAssetWriterInputPixelBufferAdaptor` 对象必须要连接到一个写入器输入 （`asset writer input`） 上。而且这个输入必须要能接收 [AVMediaTypeVideo](https://developer.apple.com/documentation/avfoundation/avmediatypevideo) 类型的媒体数据 （`media data`）。

#### 写入媒体数据 Writing Media Data

When you have configured all of the inputs needed for your asset writer, you are ready to begin writing media data. As you did with the asset reader, initiate the writing process with a call to the [startWriting](https://developer.apple.com/documentation/avfoundation/avassetwriter/1386724-startwriting) method. You then need to start a sample-writing session with a call to the [startSessionAtSourceTime:](https://developer.apple.com/documentation/avfoundation/avassetwriter/1389908-startsessionatsourcetime) method. All writing done by an asset writer has to occur within one of these sessions and the time range of each session defines the time range of media data included from within the source. For example, if your source is an asset reader that is supplying media data read from an [AVAsset](https://developer.apple.com/documentation/avfoundation/avasset) object and you don’t want to include media data from the first half of the asset, you would do the following:

当你配置完写入器 （`asset writer`） 的所有输入后，你就可以开始写入数据了。和读取器 （`assset reader`） 一样，你要调用 [startWriting](https://developer.apple.com/documentation/avfoundation/avassetwriter/1386724-startwriting) 方法来开始写入的过程。然后你需要调用 [startSessionAtSourceTime:](https://developer.apple.com/documentation/avfoundation/avassetwriter/1389908-startsessionatsourcetime) 方法来开始一个采样写入会话 （`sample-writing session`）。这个写入器的所有数据写入到在这里定义的一个会话中，而会话的时间范围用来指定了媒体数据在输入源中的时间范围。比如你有一个 [AVAsset](https://developer.apple.com/documentation/avfoundation/avasset) 对象来作为读取的数据源，而你又不想要其中前半部分的内容，你可以这样：

	CMTime halfAssetDuration = CMTimeMultiplyByFloat64(self.asset.duration, 0.5);
	[self.assetWriter startSessionAtSourceTime:halfAssetDuration];
	//Implementation continues.

Normally, to end a writing session you must call the [endSessionAtSourceTime:](https://developer.apple.com/documentation/avfoundation/avassetwriter/1389921-endsession) method. However, if your writing session goes right up to the end of your file, you can end the writing session simply by calling the [finishWriting](https://developer.apple.com/documentation/avfoundation/avassetwriter/1426644-finishwriting) method. To start up an asset writer with a single input and write all of its media data, do the following:

通常情况下，你必须要调用 [endSessionAtSourceTime:](https://developer.apple.com/documentation/avfoundation/avassetwriter/1389921-endsession) 方法来结束写入会话 （`writing session`）。但是如果你的这个写入会话 （`writing session`） 正好到了文件结束，你可以直接调用 [finishWriting](https://developer.apple.com/documentation/avfoundation/avassetwriter/1426644-finishwriting) 方法就可以结束会话。下面的代码创建了一个写入器 （`assset writer`） 和一个输入，然后输出所有的媒体数据：

	// Prepare the asset writer for writing.
	[self.assetWriter startWriting];
	// Start a sample-writing session.
	[self.assetWriter startSessionAtSourceTime:kCMTimeZero];
	// Specify the block to execute when the asset writer is ready for media data and the queue to call it on.
	[self.assetWriterInput requestMediaDataWhenReadyOnQueue:myInputSerialQueue usingBlock:^{
	     while ([self.assetWriterInput isReadyForMoreMediaData])
	     {
	          // Get the next sample buffer.
	          CMSampleBufferRef nextSampleBuffer = [self copyNextSampleBufferToWrite];
	          if (nextSampleBuffer)
	          {
	               // If it exists, append the next sample buffer to the output file.
	               [self.assetWriterInput appendSampleBuffer:nextSampleBuffer];
	               CFRelease(nextSampleBuffer);
	               nextSampleBuffer = nil;
	          }
	          else
	          {
	               // Assume that lack of a next sample buffer means the sample buffer source is out of samples and mark the input as finished.
	               [self.assetWriterInput markAsFinished];
	               break;
	          }
	     }
	}];

The `copyNextSampleBufferToWrite` method in the code above is simply a stub. The location of this stub is where you would need to insert some logic to return `CMSampleBufferRef` objects representing the media data that you want to write. One possible source of sample buffers is an asset reader output.

这里的 `copyNextSampleBufferToWriter` 方法只是一个简单的构造。这里通常用来插入你的逻辑然后返回的 `CMSampleBufferRef` 对象的媒体数据，他会被用来 写入到文件。读取器输出 （`asset reader output`） 是一个可用的采样缓冲区 （`sample buffers`） 源。

### 重新编码 Reencoding Assets

You can use an asset reader and asset writer object in tandem to convert an asset from one representation to another. Using these objects, you have more control over the conversion than you do with an `AVAssetExportSession` object. For example, you can choose which of the tracks you want to be represented in the output file, specify your own output format, or modify the asset during the conversion process. The first step in this process is just to set up your asset reader outputs and asset writer inputs as desired. After your asset reader and writer are fully configured, you start up both of them with calls to the `startReading` and `startWriting` methods, respectively. The following code snippet displays how to use a single asset writer input to write media data supplied by a single asset reader output:

你可以用一组读取器 （`asset reader`） 和写入器 （`asset writer`） 对象来将库 `asset` 内容进行转换。通过这些对象进行的转化可以或者比使用 `AVAssetExportSession` 对象更多的控制。比如你可以选择特定的轨道用来输出文件，指定自己的输出格式，或者是在转换的过程中修改库 `asset` 内容。首先你要配置好所需的 读取器的输出 （`asset reader outputs`） 和 写入器的输入 （`asset writer inputs`）。当读取器和写入器都配置好之后，你分别调用 `startReading` 和 `startWriting` 方法。下面的代码展示了如何从一个读取器输出 （`asset reader output`） 获取内容，并使用一个写入器输入（ `asset wtier input`） 写入媒体数据 （`media data`）:

	NSString *serializationQueueDescription = [NSString stringWithFormat:@"%@ serialization queue", self];
	 
	// Create a serialization queue for reading and writing.
	dispatch_queue_t serializationQueue = dispatch_queue_create([serializationQueueDescription UTF8String], NULL);
	 
	// Specify the block to execute when the asset writer is ready for media data and the queue to call it on.
	[self.assetWriterInput requestMediaDataWhenReadyOnQueue:serializationQueue usingBlock:^{
	     while ([self.assetWriterInput isReadyForMoreMediaData])
	     {
	          // Get the asset reader output's next sample buffer.
	          CMSampleBufferRef sampleBuffer = [self.assetReaderOutput copyNextSampleBuffer];
	          if (sampleBuffer != NULL)
	          {
	               // If it exists, append this sample buffer to the output file.
	               BOOL success = [self.assetWriterInput appendSampleBuffer:sampleBuffer];
	               CFRelease(sampleBuffer);
	               sampleBuffer = NULL;
	               // Check for errors that may have occurred when appending the new sample buffer.
	               if (!success && self.assetWriter.status == AVAssetWriterStatusFailed)
	               {
	                    NSError *failureError = self.assetWriter.error;
	                    //Handle the error.
	               }
	          }
	          else
	          {
	               // If the next sample buffer doesn't exist, find out why the asset reader output couldn't vend another one.
	               if (self.assetReader.status == AVAssetReaderStatusFailed)
	               {
	                    NSError *failureError = self.assetReader.error;
	                    //Handle the error here.
	               }
	               else
	               {
	                    // The asset reader output must have vended all of its samples. Mark the input as finished.
	                    [self.assetWriterInput markAsFinished];
	                    break;
	               }
	          }
	     }
	}];
	


### 整合在一起：使用读取器和写入器协同工作来对库进行重新编码 Putting It All Together: Using an Asset Reader and Writer in Tandem to Reencode an Asset

This brief code example illustrates how to use an asset reader and writer to reencode the first video and audio track of an asset into a new file. It shows how to:

- Use serialization queues to handle the asynchronous nature of reading and writing audiovisual data
- Initialize an asset reader and configure two asset reader outputs, one for audio and one for video
- Initialize an asset writer and configure two asset writer inputs, one for audio and one for video
- Use an asset reader to asynchronously supply media data to an asset writer through two different output/input combinations
- Use a dispatch group to be notified of completion of the reencoding process
- Allow a user to cancel the reencoding process once it has begun

下面的代码示例了如何使用读取器 （`asset reader`） 和 写入器 （`asset writer`） 对一个库 `asset` 中的第一个视频轨道和音频轨道进行重新编码,然后再写入到一个新的文件：

- 他使用了一个串行队列 （`serialization queues`） 来处理读取和写入音视频数据的异步操作。
- 创建了一个读取器 （`asset reader`） 并配置了两个读取输出 （`asset reader outputs`），一个用于音频另一个用于视频；
- 创建了一个写入器 （`asset writer`） 并配置了两个写入输入 （`asset writer inputs`），一个用于音频，另一个用于视频；
- 通过两个不同的输入输出组合，使用读取器 （`asset reader`） 异步读取媒体数据，然后用提供给写入器 （`asset writer`）；
- 使用 `dispatch group` 来通知重新编码过程已经完成了；
- 允许用户在重新编码的过程中取消；

Note: To focus on the most relevant code, this example omits several aspects of a complete application. To use AVFoundation, you are expected to have enough experience with Cocoa to be able to infer the missing pieces.

提示：为了关注相关的代码，这个例子忽略了一个完整应用程序所需要的很多地方。在使用 `AVFoundation` 前，请对 `Cocoa` 开发有足够的经验来弥补这些被忽略的代码。

#### 初始化的处理 Handling the Initial Setup

Before you create your asset reader and writer and configure their outputs and inputs, you need to handle some initial setup. The first part of this setup involves creating three separate serialization queues to coordinate the reading and writing process.

在你创建读取器  （`asset reader`） 和写入器 （`asset writer`） 并配置他们的输入 （`inputs`） 和 输出 （`outputs`） 前，你需要做一些初始化的设置。 第一步是要创建三个串行线程 （`serialization queues`） 来协调读取和写入的过程。

	NSString *serializationQueueDescription = [NSString stringWithFormat:@"%@ serialization queue", self];
	 
	// Create the main serialization queue.
	self.mainSerializationQueue = dispatch_queue_create([serializationQueueDescription UTF8String], NULL);
	NSString *rwAudioSerializationQueueDescription = [NSString stringWithFormat:@"%@ rw audio serialization queue", self];
	 
	// Create the serialization queue to use for reading and writing the audio data.
	self.rwAudioSerializationQueue = dispatch_queue_create([rwAudioSerializationQueueDescription UTF8String], NULL);
	NSString *rwVideoSerializationQueueDescription = [NSString stringWithFormat:@"%@ rw video serialization queue", self];
	 
	// Create the serialization queue to use for reading and writing the video data.
	self.rwVideoSerializationQueue = dispatch_queue_create([rwVideoSerializationQueueDescription UTF8String], NULL);

The main serialization queue is used to coordinate the starting and stopping of the asset reader and writer (perhaps due to cancellation) and the other two serialization queues are used to serialize the reading and writing by each output/input combination with a potential cancellation.

其中主要的串行线程 （`main serialization queue`） 用来协调读取器 （`asset reader`） 和写入器 （`asset writer`） 的开始 `starting` 和结束 `stopping`（因为可能会被取消）。 另外两个串行线程 （`serialization queues`） 是用来进行对输出 (`output`) 进行读取 (`reading`) 和 对输入 (`input`) 进行写入 (`writing`)，他们在进行过程中可能会被用户取消。

Now that you have some serialization queues, load the tracks of your asset and begin the reencoding process.

你已经有几个串行队列 （`serialization queues`） 了，现在可以从库 （`asset`） 中加载轨道 （`tracks`） 然后开始进行转码 （`reencoding`） 了。

	self.asset = <#AVAsset that you want to reencode#>;
	self.cancelled = NO;
	self.outputURL = <#NSURL representing desired output URL for file generated by asset writer#>;
	// Asynchronously load the tracks of the asset you want to read.
	[self.asset loadValuesAsynchronouslyForKeys:@[@"tracks"] completionHandler:^{
	     // Once the tracks have finished loading, dispatch the work to the main serialization queue.
	     dispatch_async(self.mainSerializationQueue, ^{
	          // Due to asynchronous nature, check to see if user has already cancelled.
	          if (self.cancelled)
	               return;
	          BOOL success = YES;
	          NSError *localError = nil;
	          // Check for success of loading the assets tracks.
	          success = ([self.asset statusOfValueForKey:@"tracks" error:&localError] == AVKeyValueStatusLoaded);
	          if (success)
	          {
	               // If the tracks loaded successfully, make sure that no file exists at the output path for the asset writer.
	               NSFileManager *fm = [NSFileManager defaultManager];
	               NSString *localOutputPath = [self.outputURL path];
	               if ([fm fileExistsAtPath:localOutputPath])
	                    success = [fm removeItemAtPath:localOutputPath error:&localError];
	          }
	          if (success)
	               success = [self setupAssetReaderAndAssetWriter:&localError];
	          if (success)
	               success = [self startAssetReaderAndWriter:&localError];
	          if (!success)
	               [self readingAndWritingDidFinishSuccessfully:success withError:localError];
	     });
	}];

When the track loading process finishes, whether successfully or not, the rest of the work is dispatched to the main serialization queue to ensure that all of this work is serialized with a potential cancellation. Now all that’s left is to implement the cancellation process and the three custom methods at the end of the previous code listing.

当轨道 （`track`） 加载完成之后，不管成功与否，剩下的工作就切换到主串行队列 （`main serialization queue`）上，要保证他们的工作的过程中可能会被随时取消。下面我们就是要来实现这个取消的过程以及上面示例代码中结尾处的三个自定义方法。

#### 初始化读取器和写入器 Initializing the Asset Reader and Writer

The custom `setupAssetReaderAndAssetWriter:` method initializes the reader and writer and configures two output/input combinations, one for an audio track and one for a video track. In this example, the audio is decompressed to Linear PCM using the asset reader and compressed back to 128 kbps AAC using the asset writer. The video is decompressed to YUV using the asset reader and compressed to H.264 using the asset writer.

在自定义方法 `setupAssetReaderAndAssetWriter: ` 中创建了一个读取器 （`reader`) 和一个写入器 (`writer`)，并配置了两个输出 `output` / 输入 `input` 组合，其中一个用于音频轨道，另一个用于视频轨道。在例子中，我们用读取器  (`asset reader`) 将音频解压到 `Linear PCM`，然后用写入器 (`asset writer`) 编码到 `128 kbps AAC`。用读取器 (`asset reader`) 将视频解码为 `YUV`，然后用写入器 (`asset writer`) 编码为 `H.264`.

	- (BOOL)setupAssetReaderAndAssetWriter:(NSError **)outError
	{
	     // Create and initialize the asset reader.
	     self.assetReader = [[AVAssetReader alloc] initWithAsset:self.asset error:outError];
	     BOOL success = (self.assetReader != nil);
	     if (success)
	     {
	          // If the asset reader was successfully initialized, do the same for the asset writer.
	          self.assetWriter = [[AVAssetWriter alloc] initWithURL:self.outputURL fileType:AVFileTypeQuickTimeMovie error:outError];
	          success = (self.assetWriter != nil);
	     }
	 
	     if (success)
	     {
	          // If the reader and writer were successfully initialized, grab the audio and video asset tracks that will be used.
	          AVAssetTrack *assetAudioTrack = nil, *assetVideoTrack = nil;
	          NSArray *audioTracks = [self.asset tracksWithMediaType:AVMediaTypeAudio];
	          if ([audioTracks count] > 0)
	               assetAudioTrack = [audioTracks objectAtIndex:0];
	          NSArray *videoTracks = [self.asset tracksWithMediaType:AVMediaTypeVideo];
	          if ([videoTracks count] > 0)
	               assetVideoTrack = [videoTracks objectAtIndex:0];
	 
	          if (assetAudioTrack)
	          {
	               // If there is an audio track to read, set the decompression settings to Linear PCM and create the asset reader output.
	               NSDictionary *decompressionAudioSettings = @{ AVFormatIDKey : [NSNumber numberWithUnsignedInt:kAudioFormatLinearPCM] };
	               self.assetReaderAudioOutput = [AVAssetReaderTrackOutput assetReaderTrackOutputWithTrack:assetAudioTrack outputSettings:decompressionAudioSettings];
	               [self.assetReader addOutput:self.assetReaderAudioOutput];
	               // Then, set the compression settings to 128kbps AAC and create the asset writer input.
	               AudioChannelLayout stereoChannelLayout = {
	                    .mChannelLayoutTag = kAudioChannelLayoutTag_Stereo,
	                    .mChannelBitmap = 0,
	                    .mNumberChannelDescriptions = 0
	               };
	               NSData *channelLayoutAsData = [NSData dataWithBytes:&stereoChannelLayout length:offsetof(AudioChannelLayout, mChannelDescriptions)];
	               NSDictionary *compressionAudioSettings = @{
	                    AVFormatIDKey         : [NSNumber numberWithUnsignedInt:kAudioFormatMPEG4AAC],
	                    AVEncoderBitRateKey   : [NSNumber numberWithInteger:128000],
	                    AVSampleRateKey       : [NSNumber numberWithInteger:44100],
	                    AVChannelLayoutKey    : channelLayoutAsData,
	                    AVNumberOfChannelsKey : [NSNumber numberWithUnsignedInteger:2]
	               };
	               self.assetWriterAudioInput = [AVAssetWriterInput assetWriterInputWithMediaType:[assetAudioTrack mediaType] outputSettings:compressionAudioSettings];
	               [self.assetWriter addInput:self.assetWriterAudioInput];
	          }
	 
	          if (assetVideoTrack)
	          {
	               // If there is a video track to read, set the decompression settings for YUV and create the asset reader output.
	               NSDictionary *decompressionVideoSettings = @{
	                    (id)kCVPixelBufferPixelFormatTypeKey     : [NSNumber numberWithUnsignedInt:kCVPixelFormatType_422YpCbCr8],
	                    (id)kCVPixelBufferIOSurfacePropertiesKey : [NSDictionary dictionary]
	               };
	               self.assetReaderVideoOutput = [AVAssetReaderTrackOutput assetReaderTrackOutputWithTrack:assetVideoTrack outputSettings:decompressionVideoSettings];
	               [self.assetReader addOutput:self.assetReaderVideoOutput];
	               CMFormatDescriptionRef formatDescription = NULL;
	               // Grab the video format descriptions from the video track and grab the first one if it exists.
	               NSArray *videoFormatDescriptions = [assetVideoTrack formatDescriptions];
	               if ([videoFormatDescriptions count] > 0)
	                    formatDescription = (__bridge CMFormatDescriptionRef)[formatDescriptions objectAtIndex:0];
	               CGSize trackDimensions = {
	                    .width = 0.0,
	                    .height = 0.0,
	               };
	               // If the video track had a format description, grab the track dimensions from there. Otherwise, grab them direcly from the track itself.
	               if (formatDescription)
	                    trackDimensions = CMVideoFormatDescriptionGetPresentationDimensions(formatDescription, false, false);
	               else
	                    trackDimensions = [assetVideoTrack naturalSize];
	               NSDictionary *compressionSettings = nil;
	               // If the video track had a format description, attempt to grab the clean aperture settings and pixel aspect ratio used by the video.
	               if (formatDescription)
	               {
	                    NSDictionary *cleanAperture = nil;
	                    NSDictionary *pixelAspectRatio = nil;
	                    CFDictionaryRef cleanApertureFromCMFormatDescription = CMFormatDescriptionGetExtension(formatDescription, kCMFormatDescriptionExtension_CleanAperture);
	                    if (cleanApertureFromCMFormatDescription)
	                    {
	                         cleanAperture = @{
	                              AVVideoCleanApertureWidthKey            : (id)CFDictionaryGetValue(cleanApertureFromCMFormatDescription, kCMFormatDescriptionKey_CleanApertureWidth),
	                              AVVideoCleanApertureHeightKey           : (id)CFDictionaryGetValue(cleanApertureFromCMFormatDescription, kCMFormatDescriptionKey_CleanApertureHeight),
	                              AVVideoCleanApertureHorizontalOffsetKey : (id)CFDictionaryGetValue(cleanApertureFromCMFormatDescription, kCMFormatDescriptionKey_CleanApertureHorizontalOffset),
	                              AVVideoCleanApertureVerticalOffsetKey   : (id)CFDictionaryGetValue(cleanApertureFromCMFormatDescription, kCMFormatDescriptionKey_CleanApertureVerticalOffset)
	                         };
	                    }
	                    CFDictionaryRef pixelAspectRatioFromCMFormatDescription = CMFormatDescriptionGetExtension(formatDescription, kCMFormatDescriptionExtension_PixelAspectRatio);
	                    if (pixelAspectRatioFromCMFormatDescription)
	                    {
	                         pixelAspectRatio = @{
	                              AVVideoPixelAspectRatioHorizontalSpacingKey : (id)CFDictionaryGetValue(pixelAspectRatioFromCMFormatDescription, kCMFormatDescriptionKey_PixelAspectRatioHorizontalSpacing),
	                              AVVideoPixelAspectRatioVerticalSpacingKey   : (id)CFDictionaryGetValue(pixelAspectRatioFromCMFormatDescription, kCMFormatDescriptionKey_PixelAspectRatioVerticalSpacing)
	                         };
	                    }
	                    // Add whichever settings we could grab from the format description to the compression settings dictionary.
	                    if (cleanAperture || pixelAspectRatio)
	                    {
	                         NSMutableDictionary *mutableCompressionSettings = [NSMutableDictionary dictionary];
	                         if (cleanAperture)
	                              [mutableCompressionSettings setObject:cleanAperture forKey:AVVideoCleanApertureKey];
	                         if (pixelAspectRatio)
	                              [mutableCompressionSettings setObject:pixelAspectRatio forKey:AVVideoPixelAspectRatioKey];
	                         compressionSettings = mutableCompressionSettings;
	                    }
	               }
	               // Create the video settings dictionary for H.264.
	               NSMutableDictionary *videoSettings = (NSMutableDictionary *) @{
	                    AVVideoCodecKey  : AVVideoCodecH264,
	                    AVVideoWidthKey  : [NSNumber numberWithDouble:trackDimensions.width],
	                    AVVideoHeightKey : [NSNumber numberWithDouble:trackDimensions.height]
	               };
	               // Put the compression settings into the video settings dictionary if we were able to grab them.
	               if (compressionSettings)
	                    [videoSettings setObject:compressionSettings forKey:AVVideoCompressionPropertiesKey];
	               // Create the asset writer input and add it to the asset writer.
	               self.assetWriterVideoInput = [AVAssetWriterInput assetWriterInputWithMediaType:[videoTrack mediaType] outputSettings:videoSettings];
	               [self.assetWriter addInput:self.assetWriterVideoInput];
	          }
	     }
	     return success;
	}
	

#### 转码 Reencoding the Asset

Provided that the asset reader and writer are successfully initialized and configured, the `startAssetReaderAndWriter:` method described in [Handling the Initial Setup](https://developer.apple.com/library/content/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/05_Export.html#//apple_ref/doc/uid/TP40010188-CH9-SW1) is called. This method is where the actual reading and writing of the asset takes place.

假如已经创建好读取器 `asset reader` 和写入器 `writer`，并将他们配置好了，那前面 `初始化处理过程` `Handling the Initial setup` 中描述的 `startAssetReaderAndWriter:` 方法会被调用。这里才是读取和写入库 `asset` 真正发生的地方：

	- (BOOL)startAssetReaderAndWriter:(NSError **)outError
	{
	     BOOL success = YES;
	     // Attempt to start the asset reader.
	     success = [self.assetReader startReading];
	     if (!success)
	          *outError = [self.assetReader error];
	     if (success)
	     {
	          // If the reader started successfully, attempt to start the asset writer.
	          success = [self.assetWriter startWriting];
	          if (!success)
	               *outError = [self.assetWriter error];
	     }
	 
	     if (success)
	     {
	          // If the asset reader and writer both started successfully, create the dispatch group where the reencoding will take place and start a sample-writing session.
	          self.dispatchGroup = dispatch_group_create();
	          [self.assetWriter startSessionAtSourceTime:kCMTimeZero];
	          self.audioFinished = NO;
	          self.videoFinished = NO;
	 
	          if (self.assetWriterAudioInput)
	          {
	               // If there is audio to reencode, enter the dispatch group before beginning the work.
	               dispatch_group_enter(self.dispatchGroup);
	               // Specify the block to execute when the asset writer is ready for audio media data, and specify the queue to call it on.
	               [self.assetWriterAudioInput requestMediaDataWhenReadyOnQueue:self.rwAudioSerializationQueue usingBlock:^{
	                    // Because the block is called asynchronously, check to see whether its task is complete.
	                    if (self.audioFinished)
	                         return;
	                    BOOL completedOrFailed = NO;
	                    // If the task isn't complete yet, make sure that the input is actually ready for more media data.
	                    while ([self.assetWriterAudioInput isReadyForMoreMediaData] && !completedOrFailed)
	                    {
	                         // Get the next audio sample buffer, and append it to the output file.
	                         CMSampleBufferRef sampleBuffer = [self.assetReaderAudioOutput copyNextSampleBuffer];
	                         if (sampleBuffer != NULL)
	                         {
	                              BOOL success = [self.assetWriterAudioInput appendSampleBuffer:sampleBuffer];
	                              CFRelease(sampleBuffer);
	                              sampleBuffer = NULL;
	                              completedOrFailed = !success;
	                         }
	                         else
	                         {
	                              completedOrFailed = YES;
	                         }
	                    }
	                    if (completedOrFailed)
	                    {
	                         // Mark the input as finished, but only if we haven't already done so, and then leave the dispatch group (since the audio work has finished).
	                         BOOL oldFinished = self.audioFinished;
	                         self.audioFinished = YES;
	                         if (oldFinished == NO)
	                         {
	                              [self.assetWriterAudioInput markAsFinished];
	                         }
	                         dispatch_group_leave(self.dispatchGroup);
	                    }
	               }];
	          }
	 
	          if (self.assetWriterVideoInput)
	          {
	               // If we had video to reencode, enter the dispatch group before beginning the work.
	               dispatch_group_enter(self.dispatchGroup);
	               // Specify the block to execute when the asset writer is ready for video media data, and specify the queue to call it on.
	               [self.assetWriterVideoInput requestMediaDataWhenReadyOnQueue:self.rwVideoSerializationQueue usingBlock:^{
	                    // Because the block is called asynchronously, check to see whether its task is complete.
	                    if (self.videoFinished)
	                         return;
	                    BOOL completedOrFailed = NO;
	                    // If the task isn't complete yet, make sure that the input is actually ready for more media data.
	                    while ([self.assetWriterVideoInput isReadyForMoreMediaData] && !completedOrFailed)
	                    {
	                         // Get the next video sample buffer, and append it to the output file.
	                         CMSampleBufferRef sampleBuffer = [self.assetReaderVideoOutput copyNextSampleBuffer];
	                         if (sampleBuffer != NULL)
	                         {
	                              BOOL success = [self.assetWriterVideoInput appendSampleBuffer:sampleBuffer];
	                              CFRelease(sampleBuffer);
	                              sampleBuffer = NULL;
	                              completedOrFailed = !success;
	                         }
	                         else
	                         {
	                              completedOrFailed = YES;
	                         }
	                    }
	                    if (completedOrFailed)
	                    {
	                         // Mark the input as finished, but only if we haven't already done so, and then leave the dispatch group (since the video work has finished).
	                         BOOL oldFinished = self.videoFinished;
	                         self.videoFinished = YES;
	                         if (oldFinished == NO)
	                         {
	                              [self.assetWriterVideoInput markAsFinished];
	                         }
	                         dispatch_group_leave(self.dispatchGroup);
	                    }
	               }];
	          }
	          // Set up the notification that the dispatch group will send when the audio and video work have both finished.
	          dispatch_group_notify(self.dispatchGroup, self.mainSerializationQueue, ^{
	               BOOL finalSuccess = YES;
	               NSError *finalError = nil;
	               // Check to see if the work has finished due to cancellation.
	               if (self.cancelled)
	               {
	                    // If so, cancel the reader and writer.
	                    [self.assetReader cancelReading];
	                    [self.assetWriter cancelWriting];
	               }
	               else
	               {
	                    // If cancellation didn't occur, first make sure that the asset reader didn't fail.
	                    if ([self.assetReader status] == AVAssetReaderStatusFailed)
	                    {
	                         finalSuccess = NO;
	                         finalError = [self.assetReader error];
	                    }
	                    // If the asset reader didn't fail, attempt to stop the asset writer and check for any errors.
	                    if (finalSuccess)
	                    {
	                         finalSuccess = [self.assetWriter finishWriting];
	                         if (!finalSuccess)
	                              finalError = [self.assetWriter error];
	                    }
	               }
	               // Call the method to handle completion, and pass in the appropriate parameters to indicate whether reencoding was successful.
	               [self readingAndWritingDidFinishSuccessfully:finalSuccess withError:finalError];
	          });
	     }
	     // Return success here to indicate whether the asset reader and writer were started successfully.
	     return success;
	}

During reencoding, the audio and video tracks are asynchronously handled on individual serialization queues to increase the overall performance of the process, but both queues are contained within the same dispatch group. By placing the work for each track within the same dispatch group, the group can send a notification when all of the work is done and the success of the reencoding process can be determined.

在转码 `reencoding` 的过程中，音频轨道和视频轨道是在不同的串行队列 `serialization queues` 中处理的，这样可以增强整体的性能。但是这两个队列是被安排在同一个分发组 `dispatch group` 中的，这样可以在所有的工作完成之后发出通知，并可以以此来确定转码过程是否成功。

#### 结束时的处理 Handling Completion

To handle the completion of the reading and writing process, the `readingAndWritingDidFinishSuccessfully:` method is called—with parameters indicating whether or not the reencoding completed successfully. If the process didn’t finish successfully, the asset reader and writer are both canceled and any UI related tasks are dispatched to the main queue.

在读写完成的时候，`readingAndWritingDidFinishSuccessfully:` 会被调用，他的参数表明了这个转码是否成功。如果转码失败，读取器 `asset raeder` 和写入器 `asset writer` 都会被取消，同时也需要在主队列 `main queue` 上进行 UI 的更新。

	- (void)readingAndWritingDidFinishSuccessfully:(BOOL)success withError:(NSError *)error
	{
	     if (!success)
	     {
	          // If the reencoding process failed, we need to cancel the asset reader and writer.
	          [self.assetReader cancelReading];
	          [self.assetWriter cancelWriting];
	          dispatch_async(dispatch_get_main_queue(), ^{
	               // Handle any UI tasks here related to failure.
	          });
	     }
	     else
	     {
	          // Reencoding was successful, reset booleans.
	          self.cancelled = NO;
	          self.videoFinished = NO;
	          self.audioFinished = NO;
	          dispatch_async(dispatch_get_main_queue(), ^{
	               // Handle any UI tasks here related to success.
	          });
	     }
	}
	

#### 取消时的处理 Handling Cancellation

Using multiple serialization queues, you can allow the user of your app to cancel the reencoding process with ease. On the main serialization queue, messages are asynchronously sent to each of the asset reencoding serialization queues to cancel their reading and writing. When these two serialization queues complete their cancellation, the dispatch group sends a notification to the main serialization queue where the cancelled property is set to `YES`. You might associate the `cancel` method from the following code listing with a button on your UI.

由于使用多个串行队列 (`serialization queues`)，这使得你的应用在取消转码 (`reencoding`) 的过程时变的很容易。在主串行队列 (`main serialization queue`) 上，消息会被异步到发送到每一个转码的队列 (`reencoding serialization queues`) 来取消他们的读取和写入过程。当两个线程队列 (`serialization queues`) 都完成了取消时，组分发器 （`dispatch group`） 会发送一个通知给主串行队列 （`main serialization queue`），那里会将 `cancelled` 属性设置为 `YES`。你已将下面这些取消操作代码和你的 UI 中的一个按钮关联：

	- (void)cancel
	{
	     // Handle cancellation asynchronously, but serialize it with the main queue.
	     dispatch_async(self.mainSerializationQueue, ^{
	          // If we had audio data to reencode, we need to cancel the audio work.
	          if (self.assetWriterAudioInput)
	          {
	               // Handle cancellation asynchronously again, but this time serialize it with the audio queue.
	               dispatch_async(self.rwAudioSerializationQueue, ^{
	                    // Update the Boolean property indicating the task is complete and mark the input as finished if it hasn't already been marked as such.
	                    BOOL oldFinished = self.audioFinished;
	                    self.audioFinished = YES;
	                    if (oldFinished == NO)
	                    {
	                         [self.assetWriterAudioInput markAsFinished];
	                    }
	                    // Leave the dispatch group since the audio work is finished now.
	                    dispatch_group_leave(self.dispatchGroup);
	               });
	          }
	 
	          if (self.assetWriterVideoInput)
	          {
	               // Handle cancellation asynchronously again, but this time serialize it with the video queue.
	               dispatch_async(self.rwVideoSerializationQueue, ^{
	                    // Update the Boolean property indicating the task is complete and mark the input as finished if it hasn't already been marked as such.
	                    BOOL oldFinished = self.videoFinished;
	                    self.videoFinished = YES;
	                    if (oldFinished == NO)
	                    {
	                         [self.assetWriterVideoInput markAsFinished];
	                    }
	                    // Leave the dispatch group, since the video work is finished now.
	                    dispatch_group_leave(self.dispatchGroup);
	               });
	          }
	          // Set the cancelled Boolean property to YES to cancel any work on the main queue as well.
	          self.cancelled = YES;
	     });
	}
	

### 库的输出设置助手 Asset Output Settings Assistant

The [AVOutputSettingsAssistant](https://developer.apple.com/documentation/avfoundation/avoutputsettingsassistant) class aids in creating `output-settings` dictionaries for an asset reader or writer. This makes setup much simpler, especially for high frame rate H264 movies that have a number of specific presets. Listing 5-1 shows an example that uses the output settings assistant to use the settings assistant.

`AVOutputSettingsAssistant` 类用来为读取器 （`asset raeder`） 和写入器 （`asset writer`） 的输出设置创建一个字典。他可以简化这个设置过程，对于 H264 高速影片这些需要一系列特殊配置参数的时候尤为有用。下面的代码展示了如何使用这个配置助手：

	AVOutputSettingsAssistant *outputSettingsAssistant = [AVOutputSettingsAssistant outputSettingsAssistantWithPreset:<some preset>];
	CMFormatDescriptionRef audioFormat = [self getAudioFormat];
	 
	if (audioFormat != NULL)
	    [outputSettingsAssistant setSourceAudioFormat:(CMAudioFormatDescriptionRef)audioFormat];
	 
	CMFormatDescriptionRef videoFormat = [self getVideoFormat];
	 
	if (videoFormat != NULL)
	    [outputSettingsAssistant setSourceVideoFormat:(CMVideoFormatDescriptionRef)videoFormat];
	 
	CMTime assetMinVideoFrameDuration = [self getMinFrameDuration];
	CMTime averageFrameDuration = [self getAvgFrameDuration]
	 
	[outputSettingsAssistant setSourceVideoAverageFrameDuration:averageFrameDuration];
	[outputSettingsAssistant setSourceVideoMinFrameDuration:assetMinVideoFrameDuration];
	 
	AVAssetWriter *assetWriter = [AVAssetWriter assetWriterWithURL:<some URL> fileType:[outputSettingsAssistant outputFileType] error:NULL];
	AVAssetWriterInput *audioInput = [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeAudio outputSettings:[outputSettingsAssistant audioSettings] sourceFormatHint:audioFormat];
	AVAssetWriterInput *videoInput = [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeVideo outputSettings:[outputSettingsAssistant videoSettings] sourceFormatHint:videoFormat];


